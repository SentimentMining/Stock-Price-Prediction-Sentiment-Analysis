{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock-pratham.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROYcLHTagHRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8bd6d8-d74d-40e2-b036-b10f82369c68"
      },
      "source": [
        "!pip install alpha_vantage\n",
        "!pip install preprocessor\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import math, random\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "import yfinance as yf\n",
        "import preprocessor as p\n",
        "import re\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: alpha_vantage in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from alpha_vantage) (3.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from alpha_vantage) (2.32.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (1.22.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->alpha_vantage) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->alpha_vantage) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->alpha_vantage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->alpha_vantage) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->alpha_vantage) (4.15.0)\n",
            "Collecting preprocessor\n",
            "  Downloading preprocessor-1.1.3.tar.gz (4.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-py3-none-any.whl size=4474 sha256=6868c5feecbc8e0ff77fb27110f9f54201b561a8d1a1db2714b0c263d385c040\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/33/31/972a156cd609581707e69298b91f0e42d349c08fefc173e19e\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ-IFSHOhGp7",
        "outputId": "24cfcd9c-3aa7-466e-c0b9-1d1f06c1bb97"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://pypi.anaconda.org/berber/simple/tweet-preprocessor/0.5.0/tweet-preprocessor-0.5.0.tar.gz (from -r requirements.txt (line 16))\n",
            "  Downloading https://pypi.anaconda.org/berber/simple/tweet-preprocessor/0.5.0/tweet-preprocessor-0.5.0.tar.gz (6.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.4.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKunoxfshJAQ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKtWj225hkAO"
      },
      "source": [
        "#**************** FUNCTIONS TO FETCH DATA ***************************\n",
        "def get_historical(quote):\n",
        "    end = datetime.now()\n",
        "    start = datetime(end.year-2,end.month,end.day)\n",
        "    data = yf.download(quote, start=start, end=end)\n",
        "    df = pd.DataFrame(data=data)\n",
        "    df.to_csv(''+quote+'.csv')\n",
        "    if(df.empty):\n",
        "        ts = TimeSeries(key='I0TWC260RP30RMO5',output_format='pandas')\n",
        "        data, meta_data = ts.get_daily_adjusted(symbol='NSE:'+quote, outputsize='full')\n",
        "        #Format df\n",
        "        #Last 2 yrs rows => 502, in ascending order => ::-1\n",
        "        data=data.head(503).iloc[::-1]\n",
        "        data=data.reset_index()\n",
        "        #Keep Required cols only\n",
        "        df=pd.DataFrame()\n",
        "        df['Date']=data['date']\n",
        "        df['Open']=data['1. open']\n",
        "        df['High']=data['2. high']\n",
        "        df['Low']=data['3. low']\n",
        "        df['Close']=data['4. close']\n",
        "        df['Adj Close']=data['5. adjusted close']\n",
        "        df['Volume']=data['6. volume']\n",
        "        df.to_csv(''+quote+'.csv',index=False)\n",
        "    return"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1_gJ7ZYhnwe"
      },
      "source": [
        "#******************** ARIMA SECTION ********************\n",
        "def ARIMA_ALGO(df):\n",
        "    uniqueVals = df[\"Code\"].unique()\n",
        "    len(uniqueVals)\n",
        "    df=df.set_index(\"Code\")\n",
        "    #for daily basis\n",
        "    def parser(x):\n",
        "        return datetime.strptime(x, '%Y-%m-%d')\n",
        "    def arima_model(train, test):\n",
        "        history = [x for x in train]\n",
        "        predictions = list()\n",
        "        for t in range(len(test)):\n",
        "            model = ARIMA(history, order=(6,1 ,0))\n",
        "            model_fit = model.fit(disp=0)\n",
        "            output = model_fit.forecast()\n",
        "            yhat = output[0]\n",
        "            predictions.append(yhat[0])\n",
        "            obs = test[t]\n",
        "            history.append(obs)\n",
        "        return predictions\n",
        "    for company in uniqueVals[:10]:\n",
        "        data=(df.loc[company,:]).reset_index()\n",
        "        data['Price'] = data['Close']\n",
        "        Quantity_date = data[['Price','Date']]\n",
        "        Quantity_date.index = Quantity_date['Date'].map(lambda x: parser(x))\n",
        "        Quantity_date['Price'] = Quantity_date['Price'].map(lambda x: float(x))\n",
        "        Quantity_date = Quantity_date.fillna(Quantity_date.bfill())\n",
        "        Quantity_date = Quantity_date.drop(['Date'],axis =1)\n",
        "        fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "        plt.plot(Quantity_date)\n",
        "        #plt.savefig('Trends.png')\n",
        "        #plt.close(fig)\n",
        "\n",
        "        quantity = Quantity_date.values\n",
        "        size = int(len(quantity) * 0.80)\n",
        "        train, test = quantity[0:size], quantity[size:len(quantity)]\n",
        "        #fit in model\n",
        "        predictions = arima_model(train, test)\n",
        "\n",
        "        #plot graph\n",
        "        fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "        plt.plot(test,label='Actual Price')\n",
        "        plt.plot(predictions,label='Predicted Price')\n",
        "        plt.legend(loc=4)\n",
        "        #plt.savefig('ARIMA.png')\n",
        "        #plt.close(fig)\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        arima_pred=predictions[-2]\n",
        "        print(\"Tomorrow's\",quote,\" Closing Price Prediction by ARIMA:\",arima_pred)\n",
        "        #rmse calculation\n",
        "        error_arima = math.sqrt(mean_squared_error(test, predictions))\n",
        "        print(\"ARIMA RMSE:\",error_arima)\n",
        "        print(\"##############################################################################\")\n",
        "        return arima_pred, error_arima"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgxhb6qhsbM"
      },
      "source": [
        "#************* LSTM SECTION **********************\n",
        "\n",
        "def LSTM_ALGO(df):\n",
        "    #Split data into training set and test set\n",
        "    dataset_train=df.iloc[0:int(0.8*len(df)),:]\n",
        "    dataset_test=df.iloc[int(0.8*len(df)):,:]\n",
        "    ############# NOTE #################\n",
        "    #TO PREDICT STOCK PRICES OF NEXT N DAYS, STORE PREVIOUS N DAYS IN MEMORY WHILE TRAINING\n",
        "    # HERE N=7\n",
        "    ###dataset_train=pd.read_csv('Google_Stock_Price_Train.csv')\n",
        "    training_set=df.iloc[:,4:5].values# 1:2, to store as numpy array else Series obj will be stored\n",
        "    #select cols using above manner to select as float64 type, view in var explorer\n",
        "\n",
        "    #Feature Scaling\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    sc=MinMaxScaler(feature_range=(0,1))#Scaled values btween 0,1\n",
        "    training_set_scaled=sc.fit_transform(training_set)\n",
        "    #In scaling, fit_transform for training, transform for test\n",
        "\n",
        "    #Creating data stucture with 7 timesteps and 1 output.\n",
        "    #7 timesteps meaning storing trends from 7 days before current day to predict 1 next output\n",
        "    X_train=[]#memory with 7 days from day i\n",
        "    y_train=[]#day i\n",
        "    for i in range(7,len(training_set_scaled)):\n",
        "        X_train.append(training_set_scaled[i-7:i,0])\n",
        "        y_train.append(training_set_scaled[i,0])\n",
        "    #Convert list to numpy arrays\n",
        "    X_train=np.array(X_train)\n",
        "    y_train=np.array(y_train)\n",
        "    X_forecast=np.array(X_train[-1,1:])\n",
        "    X_forecast=np.append(X_forecast,y_train[-1])\n",
        "    #Reshaping: Adding 3rd dimension\n",
        "    X_train=np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))#.shape 0=row,1=col\n",
        "    X_forecast=np.reshape(X_forecast, (1,X_forecast.shape[0],1))\n",
        "    #For X_train=np.reshape(no. of rows/samples, timesteps, no. of cols/features)\n",
        "\n",
        "    #Building RNN\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense\n",
        "    from keras.layers import Dropout\n",
        "    from keras.layers import LSTM\n",
        "\n",
        "    #Initialise RNN\n",
        "    regressor=Sequential()\n",
        "\n",
        "    #Add first LSTM layer\n",
        "    regressor.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))\n",
        "    #units=no. of neurons in layer\n",
        "    #input_shape=(timesteps,no. of cols/features)\n",
        "    #return_seq=True for sending recc memory. For last layer, retrun_seq=False since end of the line\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add 2nd LSTM layer\n",
        "    regressor.add(LSTM(units=50,return_sequences=True))\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add 3rd LSTM layer\n",
        "    regressor.add(LSTM(units=50,return_sequences=True))\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add 4th LSTM layer\n",
        "    regressor.add(LSTM(units=50))\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add o/p layer\n",
        "    regressor.add(Dense(units=1))\n",
        "\n",
        "    #Compile\n",
        "    regressor.compile(optimizer='adam',loss='mean_squared_error')\n",
        "\n",
        "    #Training\n",
        "    regressor.fit(X_train,y_train,epochs=25,batch_size=32 )\n",
        "    #For lstm, batch_size=power of 2\n",
        "\n",
        "    #Testing\n",
        "    ###dataset_test=pd.read_csv('Google_Stock_Price_Test.csv')\n",
        "    real_stock_price=dataset_test.iloc[:,4:5].values\n",
        "\n",
        "    #To predict, we need stock prices of 7 days before the test set\n",
        "    #So combine train and test set to get the entire data set\n",
        "    dataset_total=pd.concat((dataset_train['Close'],dataset_test['Close']),axis=0)\n",
        "    testing_set=dataset_total[ len(dataset_total) -len(dataset_test) -7: ].values\n",
        "    testing_set=testing_set.reshape(-1,1)\n",
        "    #-1=till last row, (-1,1)=>(80,1). otherwise only (80,0)\n",
        "\n",
        "    #Feature scaling\n",
        "    testing_set=sc.transform(testing_set)\n",
        "\n",
        "    #Create data structure\n",
        "    X_test=[]\n",
        "    for i in range(7,len(testing_set)):\n",
        "        X_test.append(testing_set[i-7:i,0])\n",
        "        #Convert list to numpy arrays\n",
        "    X_test=np.array(X_test)\n",
        "\n",
        "    #Reshaping: Adding 3rd dimension\n",
        "    X_test=np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "\n",
        "    #Testing Prediction\n",
        "    predicted_stock_price=regressor.predict(X_test)\n",
        "\n",
        "    #Getting original prices back from scaled values\n",
        "    predicted_stock_price=sc.inverse_transform(predicted_stock_price)\n",
        "    fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "    plt.plot(real_stock_price,label='Actual Price')\n",
        "    plt.plot(predicted_stock_price,label='Predicted Price')\n",
        "    plt.legend(loc=4)\n",
        "\n",
        "    #plt.savefig('static/LSTM.png')\n",
        "    #plt.close(fig)\n",
        "\n",
        "\n",
        "    error_lstm = math.sqrt(mean_squared_error(real_stock_price, predicted_stock_price))\n",
        "\n",
        "\n",
        "    #Forecasting Prediction\n",
        "    forecasted_stock_price=regressor.predict(X_forecast)\n",
        "\n",
        "    #Getting original prices back from scaled values\n",
        "    forecasted_stock_price=sc.inverse_transform(forecasted_stock_price)\n",
        "\n",
        "    lstm_pred=forecasted_stock_price[0,0]\n",
        "    print()\n",
        "    print(\"##############################################################################\")\n",
        "    print(\"Tomorrow's \",quote,\" Closing Price Prediction by LSTM: \",lstm_pred)\n",
        "    print(\"LSTM RMSE:\",error_lstm)\n",
        "    print(\"##############################################################################\")\n",
        "    return lstm_pred,error_lstm"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo83Wef4h1Fs"
      },
      "source": [
        "#***************** LINEAR REGRESSION SECTION ******************\n",
        "def LIN_REG_ALGO(df):\n",
        "    #No of days to be forcasted in future\n",
        "    forecast_out = int(7)\n",
        "    #Price after n days\n",
        "    df['Close after n days'] = df['Close'].shift(-forecast_out)\n",
        "    #New df with only relevant data\n",
        "    df_new=df[['Close','Close after n days']]\n",
        "\n",
        "    #Structure data for train, test & forecast\n",
        "    #lables of known data, discard last 35 rows\n",
        "    y =np.array(df_new.iloc[:-forecast_out,-1])\n",
        "    y=np.reshape(y, (-1,1))\n",
        "    #all cols of known data except lables, discard last 35 rows\n",
        "    X=np.array(df_new.iloc[:-forecast_out,0:-1])\n",
        "    #Unknown, X to be forecasted\n",
        "    X_to_be_forecasted=np.array(df_new.iloc[-forecast_out:,0:-1])\n",
        "\n",
        "    #Traning, testing to plot graphs, check accuracy\n",
        "    X_train=X[0:int(0.8*len(df)),:]\n",
        "    X_test=X[int(0.8*len(df)):,:]\n",
        "    y_train=y[0:int(0.8*len(df)),:]\n",
        "    y_test=y[int(0.8*len(df)):,:]\n",
        "\n",
        "    # Feature Scaling===Normalization\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test = sc.transform(X_test)\n",
        "\n",
        "    X_to_be_forecasted=sc.transform(X_to_be_forecasted)\n",
        "\n",
        "    #Training\n",
        "    clf = LinearRegression(n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    #Testing\n",
        "    y_test_pred=clf.predict(X_test)\n",
        "    y_test_pred=y_test_pred*(1.04)\n",
        "    import matplotlib.pyplot as plt2\n",
        "    fig = plt2.figure(figsize=(7.2,4.8),dpi=65)\n",
        "    plt2.plot(y_test,label='Actual Price' )\n",
        "    plt2.plot(y_test_pred,label='Predicted Price')\n",
        "\n",
        "    plt2.legend(loc=4)\n",
        "\n",
        "    #plt2.savefig('static/LR.png')\n",
        "    #plt2.close(fig)\n",
        "\n",
        "    error_lr = math.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "\n",
        "    #Forecasting\n",
        "    forecast_set = clf.predict(X_to_be_forecasted)\n",
        "    forecast_set=forecast_set*(1.04)\n",
        "    mean=forecast_set.mean()\n",
        "    lr_pred=forecast_set[0,0]\n",
        "    print()\n",
        "    print(\"##############################################################################\")\n",
        "    print(\"Tomorrow's \",quote,\" Closing Price Prediction by Linear Regression: \",lr_pred)\n",
        "    print(\"Linear Regression RMSE:\",error_lr)\n",
        "    print(\"##############################################################################\")\n",
        "    return df, lr_pred, forecast_set, mean, error_lr"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d_u3Gq3h4PE",
        "outputId": "a82bdddd-fb59-4bd6-f613-f8dad43eb98f"
      },
      "source": [
        "quote = input(\"Enter stock ticker symbol: \")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter stock ticker symbol: GC=F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gNF5qVkh7Dp",
        "outputId": "14712841-eddb-48ab-cd9c-0be000c2879f"
      },
      "source": [
        "get_historical(quote)\n",
        "#************** PREPROCESSING ***********************\n",
        "df = pd.read_csv(''+quote+'.csv')\n",
        "print(\"##############################################################################\")\n",
        "print(\"Today's\",quote,\"Stock Data: \")\n",
        "today_stock=df.iloc[-1:]\n",
        "print(today_stock)\n",
        "print(\"##############################################################################\")\n",
        "df = df.dropna()\n",
        "code_list=[]\n",
        "for i in range(0,len(df)):\n",
        "    code_list.append(quote)\n",
        "df2=pd.DataFrame(code_list,columns=['Code'])\n",
        "df2 = pd.concat([df2, df], axis=1)\n",
        "df=df2"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################################################################\n",
            "Today's GC=F Stock Data: \n",
            "          Price   Close              High               Low              Open  \\\n",
            "505  2025-11-13  4187.0  4192.60009765625  4173.10009765625  4174.89990234375   \n",
            "\n",
            "    Volume  \n",
            "505   9882  \n",
            "##############################################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "42Z-M_JeiBB0",
        "outputId": "a8b9dbcb-74cd-422a-a3af-b3a15e0b9d11"
      },
      "source": [
        "arima_pred, error_arima=ARIMA_ALGO(df)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Date'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1240957842.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marima_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_arima\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mARIMA_ALGO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1373922722.py\u001b[0m in \u001b[0;36mARIMA_ALGO\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mQuantity_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mQuantity_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuantity_date\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mQuantity_date\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuantity_date\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Date'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8juUdFDiZ8g"
      },
      "source": [
        "lstm_pred, error_lstm=LSTM_ALGO(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F69sy8XyiuXK"
      },
      "source": [
        " df, lr_pred, forecast_set,mean,error_lr=LIN_REG_ALGO(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaM0y8e7i8qe"
      },
      "source": [
        "#Twitter API credentials\n",
        "consumer_key= 'J8byEqCJVeadFYXaXXpxB0XPA'\n",
        "consumer_secret= 'BtCnypxBLpOcjmH40o6sdeFkVtkEVN9ETZVj0fjLyR6kBMAduJ'\n",
        "\n",
        "access_token='593352028-586dxldnHIrPKM2aSfsq0yJBwe9ulEQNk6LWMlln'\n",
        "access_token_secret='JOnyIQx4oiR96Sp72vMQwZFJRdoOy2dtCXZqS7kbyrV2k'\n",
        "\n",
        "num_of_tweets = int(300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEUfQlaujCad"
      },
      "source": [
        "#Setting up modules for Tweepy\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "class Tweet(object):\n",
        "\n",
        "    def __init__(self, content, polarity):\n",
        "        self.content = content\n",
        "        self.polarity = polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjyLybuajHxW"
      },
      "source": [
        "#------------SENTIMENT ANALYSIS------------------\n",
        "def retrieving_tweets_polarity(symbol):\n",
        "    stock_ticker_map = pd.read_csv('Yahoo-Finance-Ticker-Symbols.csv')\n",
        "    stock_full_form = stock_ticker_map[stock_ticker_map['Ticker']==symbol]\n",
        "    symbol = stock_full_form['Name'].to_list()[0][0:12]\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    user = tweepy.API(auth)\n",
        "\n",
        "    tweets = tweepy.Cursor(user.search, q=symbol, tweet_mode='extended', lang='en',exclude_replies=True).items(num_of_tweets)\n",
        "\n",
        "    tweet_list = [] #List of tweets alongside polarity\n",
        "    global_polarity = 0 #Polarity of all tweets === Sum of polarities of individual tweets\n",
        "    tw_list=[] #List of tweets only => to be displayed on web page\n",
        "    #Count Positive, Negative to plot pie chart\n",
        "    pos=0 #Num of pos tweets\n",
        "    neg=1 #Num of negative tweets\n",
        "    for tweet in tweets:\n",
        "        count=20 #Num of tweets to be displayed on web page\n",
        "        #Convert to Textblob format for assigning polarity\n",
        "        tw2 = tweet.full_text\n",
        "        tw = tweet.full_text\n",
        "        #Clean\n",
        "        tw=p.clean(tw)\n",
        "        #print(\"-------------------------------CLEANED TWEET-----------------------------\")\n",
        "        #print(tw)\n",
        "        #Replace &amp; by &\n",
        "        tw=re.sub('&amp;','&',tw)\n",
        "        #Remove :\n",
        "        tw=re.sub(':','',tw)\n",
        "        #print(\"-------------------------------TWEET AFTER REGEX MATCHING-----------------------------\")\n",
        "        #print(tw)\n",
        "        #Remove Emojis and Hindi Characters\n",
        "        tw=tw.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "        #print(\"-------------------------------TWEET AFTER REMOVING NON ASCII CHARS-----------------------------\")\n",
        "        #print(tw)\n",
        "        blob = TextBlob(tw)\n",
        "        polarity = 0 #Polarity of single individual tweet\n",
        "        for sentence in blob.sentences:\n",
        "\n",
        "            polarity += sentence.sentiment.polarity\n",
        "            if polarity>0:\n",
        "                pos=pos+1\n",
        "            if polarity<0:\n",
        "                neg=neg+1\n",
        "\n",
        "            global_polarity += sentence.sentiment.polarity\n",
        "        if count > 0:\n",
        "            tw_list.append(tw2)\n",
        "\n",
        "        tweet_list.append(Tweet(tw, polarity))\n",
        "        count=count-1\n",
        "    if len(tweet_list) != 0:\n",
        "        global_polarity = global_polarity / len(tweet_list)\n",
        "    else:\n",
        "        global_polarity = global_polarity\n",
        "    neutral=num_of_tweets-pos-neg\n",
        "    if neutral<0:\n",
        "      neg=neg+neutral\n",
        "      neutral=20\n",
        "    print()\n",
        "    print(\"##############################################################################\")\n",
        "    print(\"Positive Tweets :\",pos,\"Negative Tweets :\",neg,\"Neutral Tweets :\",neutral)\n",
        "    print(\"##############################################################################\")\n",
        "    labels=['Positive','Negative','Neutral']\n",
        "    sizes = [pos,neg,neutral]\n",
        "    explode = (0, 0, 0)\n",
        "    fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "    fig1, ax1 = plt.subplots(figsize=(7.2,4.8),dpi=65)\n",
        "    ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
        "    ax1.axis('equal')\n",
        "    plt.tight_layout()\n",
        "    #plt.savefig('static/SA.png')\n",
        "    #plt.close(fig)\n",
        "    plt.show()\n",
        "    if global_polarity>0:\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        print(\"Tweets Polarity: Overall Positive\")\n",
        "        print(\"##############################################################################\")\n",
        "        tw_pol=\"Overall Positive\"\n",
        "    else:\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        print(\"Tweets Polarity: Overall Negative\")\n",
        "        print(\"##############################################################################\")\n",
        "        tw_pol=\"Overall Negative\"\n",
        "\n",
        "\n",
        "    return global_polarity,tw_list,tw_pol,pos,neg,neutral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjlxnFV9jeHX"
      },
      "source": [
        "#---------RECOMENDATIONS BASED ON TWEETS & Models-------------------\n",
        "def recommending(df, global_polarity,today_stock,mean):\n",
        "    if today_stock.iloc[-1]['Close'] < mean:\n",
        "        if global_polarity > 0:\n",
        "            idea=\"RISE\"\n",
        "            decision=\"BUY\"\n",
        "            print()\n",
        "            print(\"##############################################################################\")\n",
        "            print(\"According to the ML Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected => \",decision)\n",
        "        elif global_polarity <= 0:\n",
        "            idea=\"FALL\"\n",
        "            decision=\"SELL\"\n",
        "            print()\n",
        "            print(\"##############################################################################\")\n",
        "            print(\"According to the ML Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected => \",decision)\n",
        "    else:\n",
        "        idea=\"FALL\"\n",
        "        decision=\"SELL\"\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        print(\"According to the ML Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected => \",decision)\n",
        "    return idea, decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25exPfW-jh9P"
      },
      "source": [
        "#Showing sentiment analysis\n",
        "polarity,tw_list,tw_pol,pos,neg,neutral = retrieving_tweets_polarity(quote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdoQ85R_jkyL"
      },
      "source": [
        "idea, decision=recommending(df, polarity,today_stock,mean)\n",
        "print()\n",
        "print(\"Forecasted Prices for Next 7 days:\")\n",
        "print(forecast_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSjHxjD0j0JN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}