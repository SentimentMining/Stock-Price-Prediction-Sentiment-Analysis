{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock-pratham.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROYcLHTagHRw",
        "outputId": "ace4a10b-8bdd-4dd3-de61-6e7f2dc3b6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "!pip install alpha_vantage\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import math, random\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "import yfinance as yf\n",
        "import preprocessor as p\n",
        "import re\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'alpha_vantage'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4806552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malpha_vantage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marima_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alpha_vantage'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ-IFSHOhGp7"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKunoxfshJAQ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKtWj225hkAO"
      },
      "source": [
        "#**************** FUNCTIONS TO FETCH DATA ***************************\n",
        "def get_historical(quote):\n",
        "    end = datetime.now()\n",
        "    start = datetime(end.year-2,end.month,end.day)\n",
        "    data = yf.download(quote, start=start, end=end)\n",
        "    df = pd.DataFrame(data=data)\n",
        "    df.to_csv(''+quote+'.csv')\n",
        "    if(df.empty):\n",
        "        ts = TimeSeries(key='I0TWC260RP30RMO5',output_format='pandas')\n",
        "        data, meta_data = ts.get_daily_adjusted(symbol='NSE:'+quote, outputsize='full')\n",
        "        #Format df\n",
        "        #Last 2 yrs rows => 502, in ascending order => ::-1\n",
        "        data=data.head(503).iloc[::-1]\n",
        "        data=data.reset_index()\n",
        "        #Keep Required cols only\n",
        "        df=pd.DataFrame()\n",
        "        df['Date']=data['date']\n",
        "        df['Open']=data['1. open']\n",
        "        df['High']=data['2. high']\n",
        "        df['Low']=data['3. low']\n",
        "        df['Close']=data['4. close']\n",
        "        df['Adj Close']=data['5. adjusted close']\n",
        "        df['Volume']=data['6. volume']\n",
        "        df.to_csv(''+quote+'.csv',index=False)\n",
        "    return"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1_gJ7ZYhnwe"
      },
      "source": [
        "#******************** ARIMA SECTION ********************\n",
        "def ARIMA_ALGO(df):\n",
        "    uniqueVals = df[\"Code\"].unique()\n",
        "    len(uniqueVals)\n",
        "    df=df.set_index(\"Code\")\n",
        "    #for daily basis\n",
        "    def parser(x):\n",
        "        return datetime.strptime(x, '%Y-%m-%d')\n",
        "    def arima_model(train, test):\n",
        "        history = [x for x in train]\n",
        "        predictions = list()\n",
        "        for t in range(len(test)):\n",
        "            model = ARIMA(history, order=(6,1 ,0))\n",
        "            model_fit = model.fit(disp=0)\n",
        "            output = model_fit.forecast()\n",
        "            yhat = output[0]\n",
        "            predictions.append(yhat[0])\n",
        "            obs = test[t]\n",
        "            history.append(obs)\n",
        "        return predictions\n",
        "    for company in uniqueVals[:10]:\n",
        "        data=(df.loc[company,:]).reset_index()\n",
        "        data['Price'] = data['Close']\n",
        "        Quantity_date = data[['Price','Date']]\n",
        "        Quantity_date.index = Quantity_date['Date'].map(lambda x: parser(x))\n",
        "        Quantity_date['Price'] = Quantity_date['Price'].map(lambda x: float(x))\n",
        "        Quantity_date = Quantity_date.fillna(Quantity_date.bfill())\n",
        "        Quantity_date = Quantity_date.drop(['Date'],axis =1)\n",
        "        fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "        plt.plot(Quantity_date)\n",
        "        #plt.savefig('Trends.png')\n",
        "        #plt.close(fig)\n",
        "\n",
        "        quantity = Quantity_date.values\n",
        "        size = int(len(quantity) * 0.80)\n",
        "        train, test = quantity[0:size], quantity[size:len(quantity)]\n",
        "        #fit in model\n",
        "        predictions = arima_model(train, test)\n",
        "\n",
        "        #plot graph\n",
        "        fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "        plt.plot(test,label='Actual Price')\n",
        "        plt.plot(predictions,label='Predicted Price')\n",
        "        plt.legend(loc=4)\n",
        "        #plt.savefig('ARIMA.png')\n",
        "        #plt.close(fig)\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        arima_pred=predictions[-2]\n",
        "        print(\"Tomorrow's\",quote,\" Closing Price Prediction by ARIMA:\",arima_pred)\n",
        "        #rmse calculation\n",
        "        error_arima = math.sqrt(mean_squared_error(test, predictions))\n",
        "        print(\"ARIMA RMSE:\",error_arima)\n",
        "        print(\"##############################################################################\")\n",
        "        return arima_pred, error_arima"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgxhb6qhsbM"
      },
      "source": [
        "#************* LSTM SECTION **********************\n",
        "\n",
        "def LSTM_ALGO(df):\n",
        "    #Split data into training set and test set\n",
        "    dataset_train=df.iloc[0:int(0.8*len(df)),:]\n",
        "    dataset_test=df.iloc[int(0.8*len(df)):,:]\n",
        "    ############# NOTE #################\n",
        "    #TO PREDICT STOCK PRICES OF NEXT N DAYS, STORE PREVIOUS N DAYS IN MEMORY WHILE TRAINING\n",
        "    # HERE N=7\n",
        "    ###dataset_train=pd.read_csv('Google_Stock_Price_Train.csv')\n",
        "    training_set=df.iloc[:,4:5].values# 1:2, to store as numpy array else Series obj will be stored\n",
        "    #select cols using above manner to select as float64 type, view in var explorer\n",
        "\n",
        "    #Feature Scaling\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    sc=MinMaxScaler(feature_range=(0,1))#Scaled values btween 0,1\n",
        "    training_set_scaled=sc.fit_transform(training_set)\n",
        "    #In scaling, fit_transform for training, transform for test\n",
        "\n",
        "    #Creating data stucture with 7 timesteps and 1 output.\n",
        "    #7 timesteps meaning storing trends from 7 days before current day to predict 1 next output\n",
        "    X_train=[]#memory with 7 days from day i\n",
        "    y_train=[]#day i\n",
        "    for i in range(7,len(training_set_scaled)):\n",
        "        X_train.append(training_set_scaled[i-7:i,0])\n",
        "        y_train.append(training_set_scaled[i,0])\n",
        "    #Convert list to numpy arrays\n",
        "    X_train=np.array(X_train)\n",
        "    y_train=np.array(y_train)\n",
        "    X_forecast=np.array(X_train[-1,1:])\n",
        "    X_forecast=np.append(X_forecast,y_train[-1])\n",
        "    #Reshaping: Adding 3rd dimension\n",
        "    X_train=np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))#.shape 0=row,1=col\n",
        "    X_forecast=np.reshape(X_forecast, (1,X_forecast.shape[0],1))\n",
        "    #For X_train=np.reshape(no. of rows/samples, timesteps, no. of cols/features)\n",
        "\n",
        "    #Building RNN\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense\n",
        "    from keras.layers import Dropout\n",
        "    from keras.layers import LSTM\n",
        "\n",
        "    #Initialise RNN\n",
        "    regressor=Sequential()\n",
        "\n",
        "    #Add first LSTM layer\n",
        "    regressor.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))\n",
        "    #units=no. of neurons in layer\n",
        "    #input_shape=(timesteps,no. of cols/features)\n",
        "    #return_seq=True for sending recc memory. For last layer, retrun_seq=False since end of the line\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add 2nd LSTM layer\n",
        "    regressor.add(LSTM(units=50,return_sequences=True))\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add 3rd LSTM layer\n",
        "    regressor.add(LSTM(units=50,return_sequences=True))\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add 4th LSTM layer\n",
        "    regressor.add(LSTM(units=50))\n",
        "    regressor.add(Dropout(0.1))\n",
        "\n",
        "    #Add o/p layer\n",
        "    regressor.add(Dense(units=1))\n",
        "\n",
        "    #Compile\n",
        "    regressor.compile(optimizer='adam',loss='mean_squared_error')\n",
        "\n",
        "    #Training\n",
        "    regressor.fit(X_train,y_train,epochs=25,batch_size=32 )\n",
        "    #For lstm, batch_size=power of 2\n",
        "\n",
        "    #Testing\n",
        "    ###dataset_test=pd.read_csv('Google_Stock_Price_Test.csv')\n",
        "    real_stock_price=dataset_test.iloc[:,4:5].values\n",
        "\n",
        "    #To predict, we need stock prices of 7 days before the test set\n",
        "    #So combine train and test set to get the entire data set\n",
        "    dataset_total=pd.concat((dataset_train['Close'],dataset_test['Close']),axis=0)\n",
        "    testing_set=dataset_total[ len(dataset_total) -len(dataset_test) -7: ].values\n",
        "    testing_set=testing_set.reshape(-1,1)\n",
        "    #-1=till last row, (-1,1)=>(80,1). otherwise only (80,0)\n",
        "\n",
        "    #Feature scaling\n",
        "    testing_set=sc.transform(testing_set)\n",
        "\n",
        "    #Create data structure\n",
        "    X_test=[]\n",
        "    for i in range(7,len(testing_set)):\n",
        "        X_test.append(testing_set[i-7:i,0])\n",
        "        #Convert list to numpy arrays\n",
        "    X_test=np.array(X_test)\n",
        "\n",
        "    #Reshaping: Adding 3rd dimension\n",
        "    X_test=np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "\n",
        "    #Testing Prediction\n",
        "    predicted_stock_price=regressor.predict(X_test)\n",
        "\n",
        "    #Getting original prices back from scaled values\n",
        "    predicted_stock_price=sc.inverse_transform(predicted_stock_price)\n",
        "    fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "    plt.plot(real_stock_price,label='Actual Price')\n",
        "    plt.plot(predicted_stock_price,label='Predicted Price')\n",
        "    plt.legend(loc=4)\n",
        "\n",
        "    #plt.savefig('static/LSTM.png')\n",
        "    #plt.close(fig)\n",
        "\n",
        "\n",
        "    error_lstm = math.sqrt(mean_squared_error(real_stock_price, predicted_stock_price))\n",
        "\n",
        "\n",
        "    #Forecasting Prediction\n",
        "    forecasted_stock_price=regressor.predict(X_forecast)\n",
        "\n",
        "    #Getting original prices back from scaled values\n",
        "    forecasted_stock_price=sc.inverse_transform(forecasted_stock_price)\n",
        "\n",
        "    lstm_pred=forecasted_stock_price[0,0]\n",
        "    print()\n",
        "    print(\"##############################################################################\")\n",
        "    print(\"Tomorrow's \",quote,\" Closing Price Prediction by LSTM: \",lstm_pred)\n",
        "    print(\"LSTM RMSE:\",error_lstm)\n",
        "    print(\"##############################################################################\")\n",
        "    return lstm_pred,error_lstm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo83Wef4h1Fs"
      },
      "source": [
        "#***************** LINEAR REGRESSION SECTION ******************\n",
        "def LIN_REG_ALGO(df):\n",
        "    #No of days to be forcasted in future\n",
        "    forecast_out = int(7)\n",
        "    #Price after n days\n",
        "    df['Close after n days'] = df['Close'].shift(-forecast_out)\n",
        "    #New df with only relevant data\n",
        "    df_new=df[['Close','Close after n days']]\n",
        "\n",
        "    #Structure data for train, test & forecast\n",
        "    #lables of known data, discard last 35 rows\n",
        "    y =np.array(df_new.iloc[:-forecast_out,-1])\n",
        "    y=np.reshape(y, (-1,1))\n",
        "    #all cols of known data except lables, discard last 35 rows\n",
        "    X=np.array(df_new.iloc[:-forecast_out,0:-1])\n",
        "    #Unknown, X to be forecasted\n",
        "    X_to_be_forecasted=np.array(df_new.iloc[-forecast_out:,0:-1])\n",
        "\n",
        "    #Traning, testing to plot graphs, check accuracy\n",
        "    X_train=X[0:int(0.8*len(df)),:]\n",
        "    X_test=X[int(0.8*len(df)):,:]\n",
        "    y_train=y[0:int(0.8*len(df)),:]\n",
        "    y_test=y[int(0.8*len(df)):,:]\n",
        "\n",
        "    # Feature Scaling===Normalization\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test = sc.transform(X_test)\n",
        "\n",
        "    X_to_be_forecasted=sc.transform(X_to_be_forecasted)\n",
        "\n",
        "    #Training\n",
        "    clf = LinearRegression(n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    #Testing\n",
        "    y_test_pred=clf.predict(X_test)\n",
        "    y_test_pred=y_test_pred*(1.04)\n",
        "    import matplotlib.pyplot as plt2\n",
        "    fig = plt2.figure(figsize=(7.2,4.8),dpi=65)\n",
        "    plt2.plot(y_test,label='Actual Price' )\n",
        "    plt2.plot(y_test_pred,label='Predicted Price')\n",
        "\n",
        "    plt2.legend(loc=4)\n",
        "\n",
        "    #plt2.savefig('static/LR.png')\n",
        "    #plt2.close(fig)\n",
        "\n",
        "    error_lr = math.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "\n",
        "    #Forecasting\n",
        "    forecast_set = clf.predict(X_to_be_forecasted)\n",
        "    forecast_set=forecast_set*(1.04)\n",
        "    mean=forecast_set.mean()\n",
        "    lr_pred=forecast_set[0,0]\n",
        "    print()\n",
        "    print(\"##############################################################################\")\n",
        "    print(\"Tomorrow's \",quote,\" Closing Price Prediction by Linear Regression: \",lr_pred)\n",
        "    print(\"Linear Regression RMSE:\",error_lr)\n",
        "    print(\"##############################################################################\")\n",
        "    return df, lr_pred, forecast_set, mean, error_lr"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d_u3Gq3h4PE"
      },
      "source": [
        "quote = input(\"Enter stock ticker symbol: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gNF5qVkh7Dp"
      },
      "source": [
        "get_historical(quote)\n",
        "#************** PREPROCESSING ***********************\n",
        "df = pd.read_csv(''+quote+'.csv')\n",
        "print(\"##############################################################################\")\n",
        "print(\"Today's\",quote,\"Stock Data: \")\n",
        "today_stock=df.iloc[-1:]\n",
        "print(today_stock)\n",
        "print(\"##############################################################################\")\n",
        "df = df.dropna()\n",
        "code_list=[]\n",
        "for i in range(0,len(df)):\n",
        "    code_list.append(quote)\n",
        "df2=pd.DataFrame(code_list,columns=['Code'])\n",
        "df2 = pd.concat([df2, df], axis=1)\n",
        "df=df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Z-M_JeiBB0"
      },
      "source": [
        "arima_pred, error_arima=ARIMA_ALGO(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8juUdFDiZ8g"
      },
      "source": [
        "lstm_pred, error_lstm=LSTM_ALGO(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F69sy8XyiuXK"
      },
      "source": [
        " df, lr_pred, forecast_set,mean,error_lr=LIN_REG_ALGO(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaM0y8e7i8qe"
      },
      "source": [
        "#Twitter API credentials\n",
        "consumer_key= 'J8byEqCJVeadFYXaXXpxB0XPA'\n",
        "consumer_secret= 'BtCnypxBLpOcjmH40o6sdeFkVtkEVN9ETZVj0fjLyR6kBMAduJ'\n",
        "\n",
        "access_token='593352028-586dxldnHIrPKM2aSfsq0yJBwe9ulEQNk6LWMlln'\n",
        "access_token_secret='JOnyIQx4oiR96Sp72vMQwZFJRdoOy2dtCXZqS7kbyrV2k'\n",
        "\n",
        "num_of_tweets = int(300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEUfQlaujCad"
      },
      "source": [
        "#Setting up modules for Tweepy\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "class Tweet(object):\n",
        "\n",
        "    def __init__(self, content, polarity):\n",
        "        self.content = content\n",
        "        self.polarity = polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjyLybuajHxW"
      },
      "source": [
        "#------------SENTIMENT ANALYSIS------------------\n",
        "def retrieving_tweets_polarity(symbol):\n",
        "    stock_ticker_map = pd.read_csv('Yahoo-Finance-Ticker-Symbols.csv')\n",
        "    stock_full_form = stock_ticker_map[stock_ticker_map['Ticker']==symbol]\n",
        "    symbol = stock_full_form['Name'].to_list()[0][0:12]\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    user = tweepy.API(auth)\n",
        "\n",
        "    tweets = tweepy.Cursor(user.search, q=symbol, tweet_mode='extended', lang='en',exclude_replies=True).items(num_of_tweets)\n",
        "\n",
        "    tweet_list = [] #List of tweets alongside polarity\n",
        "    global_polarity = 0 #Polarity of all tweets === Sum of polarities of individual tweets\n",
        "    tw_list=[] #List of tweets only => to be displayed on web page\n",
        "    #Count Positive, Negative to plot pie chart\n",
        "    pos=0 #Num of pos tweets\n",
        "    neg=1 #Num of negative tweets\n",
        "    for tweet in tweets:\n",
        "        count=20 #Num of tweets to be displayed on web page\n",
        "        #Convert to Textblob format for assigning polarity\n",
        "        tw2 = tweet.full_text\n",
        "        tw = tweet.full_text\n",
        "        #Clean\n",
        "        tw=p.clean(tw)\n",
        "        #print(\"-------------------------------CLEANED TWEET-----------------------------\")\n",
        "        #print(tw)\n",
        "        #Replace &amp; by &\n",
        "        tw=re.sub('&amp;','&',tw)\n",
        "        #Remove :\n",
        "        tw=re.sub(':','',tw)\n",
        "        #print(\"-------------------------------TWEET AFTER REGEX MATCHING-----------------------------\")\n",
        "        #print(tw)\n",
        "        #Remove Emojis and Hindi Characters\n",
        "        tw=tw.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "        #print(\"-------------------------------TWEET AFTER REMOVING NON ASCII CHARS-----------------------------\")\n",
        "        #print(tw)\n",
        "        blob = TextBlob(tw)\n",
        "        polarity = 0 #Polarity of single individual tweet\n",
        "        for sentence in blob.sentences:\n",
        "\n",
        "            polarity += sentence.sentiment.polarity\n",
        "            if polarity>0:\n",
        "                pos=pos+1\n",
        "            if polarity<0:\n",
        "                neg=neg+1\n",
        "\n",
        "            global_polarity += sentence.sentiment.polarity\n",
        "        if count > 0:\n",
        "            tw_list.append(tw2)\n",
        "\n",
        "        tweet_list.append(Tweet(tw, polarity))\n",
        "        count=count-1\n",
        "    if len(tweet_list) != 0:\n",
        "        global_polarity = global_polarity / len(tweet_list)\n",
        "    else:\n",
        "        global_polarity = global_polarity\n",
        "    neutral=num_of_tweets-pos-neg\n",
        "    if neutral<0:\n",
        "      neg=neg+neutral\n",
        "      neutral=20\n",
        "    print()\n",
        "    print(\"##############################################################################\")\n",
        "    print(\"Positive Tweets :\",pos,\"Negative Tweets :\",neg,\"Neutral Tweets :\",neutral)\n",
        "    print(\"##############################################################################\")\n",
        "    labels=['Positive','Negative','Neutral']\n",
        "    sizes = [pos,neg,neutral]\n",
        "    explode = (0, 0, 0)\n",
        "    fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
        "    fig1, ax1 = plt.subplots(figsize=(7.2,4.8),dpi=65)\n",
        "    ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
        "    ax1.axis('equal')\n",
        "    plt.tight_layout()\n",
        "    #plt.savefig('static/SA.png')\n",
        "    #plt.close(fig)\n",
        "    plt.show()\n",
        "    if global_polarity>0:\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        print(\"Tweets Polarity: Overall Positive\")\n",
        "        print(\"##############################################################################\")\n",
        "        tw_pol=\"Overall Positive\"\n",
        "    else:\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        print(\"Tweets Polarity: Overall Negative\")\n",
        "        print(\"##############################################################################\")\n",
        "        tw_pol=\"Overall Negative\"\n",
        "\n",
        "\n",
        "    return global_polarity,tw_list,tw_pol,pos,neg,neutral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjlxnFV9jeHX"
      },
      "source": [
        "#---------RECOMENDATIONS BASED ON TWEETS & Models-------------------\n",
        "def recommending(df, global_polarity,today_stock,mean):\n",
        "    if today_stock.iloc[-1]['Close'] < mean:\n",
        "        if global_polarity > 0:\n",
        "            idea=\"RISE\"\n",
        "            decision=\"BUY\"\n",
        "            print()\n",
        "            print(\"##############################################################################\")\n",
        "            print(\"According to the ML Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected => \",decision)\n",
        "        elif global_polarity <= 0:\n",
        "            idea=\"FALL\"\n",
        "            decision=\"SELL\"\n",
        "            print()\n",
        "            print(\"##############################################################################\")\n",
        "            print(\"According to the ML Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected => \",decision)\n",
        "    else:\n",
        "        idea=\"FALL\"\n",
        "        decision=\"SELL\"\n",
        "        print()\n",
        "        print(\"##############################################################################\")\n",
        "        print(\"According to the ML Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected => \",decision)\n",
        "    return idea, decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25exPfW-jh9P"
      },
      "source": [
        "#Showing sentiment analysis\n",
        "polarity,tw_list,tw_pol,pos,neg,neutral = retrieving_tweets_polarity(quote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdoQ85R_jkyL"
      },
      "source": [
        "idea, decision=recommending(df, polarity,today_stock,mean)\n",
        "print()\n",
        "print(\"Forecasted Prices for Next 7 days:\")\n",
        "print(forecast_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSjHxjD0j0JN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}